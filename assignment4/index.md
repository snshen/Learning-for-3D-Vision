# Assignment 4: Neural Surfaces
Number of late days used:
<img src="./images/zero.png"  width="3%">

##  1. Sphere Tracing (30pts)

`part_1.gif` generated by my code:

![Torus](images/part_1.gif)

The pseudocode for my sphere tracing process is provided below:

![Pseudo](images/pseudo.png)

In order to implement sphere tracing, I first initialized the points to be the provided origins (1). Then the algorithm iterates by max_iters which is provided by the configuration file used (2). During ever iteration, the distance between the points to the closest surface is obtained using the implicit_fn (3) and the points position are moved along their associated directions by the calculated distances using einsum (4). Finally, the distance of each resulting point to the closest surface is used to create a mask where any point with a distance less than self.epsilon is set to true (6,7). Note, that the self.epsilon is set during SphereTracingRenderer initialization and is default set as 1. Finally, the final points and associated mask is returned (8).

##  2. Optimizing a Neural SDF (30pts)

Visualizes of the input point cloud used for training and the NeuralSurface prediction. 

|Input Point Cloud|Model Output|
|:-:|:-:|
|![Bunny geometry](images/part_2_input_fin.gif)|![Bunny geometry](images/part_2_fin.gif)|

Brief descriptions of your MLP and eikonal loss:

The MLP I used was based on the NeRF model and had the following architecture

<img src="./images/SDF_MLP.png"  width="50%">

The input vectors are shown in green, intermediate hidden layers are shown in blue, output vectors are shown in orange, and the number inside each block signifies the vector’s dimension. All layers are standard fully-connected layers, black arrows indicate layers with ReLU activations, orange arrows indicate layers with sigmoid activation, and “+” denotes vector concatenation. The positional encoding of the input location (γ(x)) is passed through 6 fully-connected ReLU layers, each with 128 channels. I also took inspiration from the DeepSDF architecture and include a skip connection that concatenates this input to the to the activation of the middle layer. Note that the number of harmonics in the embedding, layer dimensions, and number of layers can all be modified by the config file.

For my eikonal loss, I used the absolute value of the difference between the L2 norm of the gradient and 1 (which is the taget value for the norm). For a batch input, the equation is defined as loss = mean(abs(norm(gradients-1)))

##  3. VolSDF (20 pts)

* **Color Prediction**: Extend the the `NeuralSurface` class to predict per-point color. You may need to define a new MLP (a just a few new layers depending on how you implemented Q2). You should then implement the `get_color` and `get_distance_color` functions.

Using section 3.1 of the [VolSDF Paper](https://arxiv.org/pdf/2106.12052.pdf), we implement their for formula converting signed distance to density which is 

<p align="center">
  <img src="images/eq1.png" width=15%/>
</p>
<p align="center">
  <img src="images/eq2.png" width=25%/>
</p>

In this equation, α and β are defined as learnable parameters, however in our implementation we will be setting these values. β defines the standard deviation of the 0 mean Cumulative Distribution Function (CDF, denotes as Ψ_β) which is used to describe a homogeneous object along with with a constant density scaling factor α. Looking at the equation, we can intuitively think of higher α as higher overall density and higher β as greater smoothness in the output. Given this understanding we can draw the following conclusions:

1. How does high `beta` bias your learned SDF? What about low `beta`?

A higher β causes the the SDF surface to be extremely smooth while a lower β to have sharp boundaries.

2. Would an SDF be easier to train with volume rendering and low `beta` or high `beta`? Why?

A SDF may be more easily trained with a higher value of β since the predicte surface distribution is spread out across a larger area, giving the for the output a higher chance of being close to or overlap with the ground truth and therefore allowing the model to more smoothly convergence while. The wide distribution also ensures that the model will be less likely to overfit to any given sample. On the other hand, a lower value of β could cause the gradients of the model to be unstable during training as the gradients could be extremely large and distincly different for each sample causing the training loss to blow up.

3. Would you be more likely to learn an accurate surface with high `beta` or low `beta`? Why?

A SDF may be more accurate with a lower value of β since the predicte surface distribution is tight and thus able to better represent sharp edges and fine details.  While a higher value of β may converge more smoothly, as mentioned in the previouse point, the larger distribution means that even if the distribution is centered around the ground truth, the higher standard deviation in the model will cause it tp be less accurate.

My best results were acheived with the default configurations and are as follows: 

![Bulldozer geometry](images/part_3_geometry_05.gif) ![Bulldozer color](images/part_3_05.gif)


I had run my model with the default configurations. For reference, the MLP I had the following architecture:

<img src="./images/COLOR_SDF.png"  width="50%">

(Details on how to interprest the visual can be found in section 2.)

Then, I experimented some of my parameters, An example of how I tuned my parameters is visualized below where I am varying the parameters of β:

|β Values|Geometry|Full Visualization|
|:-:|:-:|:-:|
|0.05|![Bunny geometry](images/part_3_geometry_05.gif)|![Bunny geometry](images/part_3_05.gif)|
|0.1|![Bunny geometry](images/part_3_geometry_1.gif)|![Bunny geometry](images/part_3_1.gif)|
|0.5|![Bunny geometry](images/part_3_geometry_5.gif)|![Bunny geometry](images/part_3_5.gif)|

As seen, the larger the β value, the blurrier the final imaage. Interestingly, with a larger β, some random color artifacts also appeared. Note that with my architecture, β of less than 0.05 would begin training reasonably but the loss would randomly explode causing the model to start diverging, even with gradient clipping in place. I theorized that a lower β may require a lower and/or faster decaying lr.

## 4. Phong Relighting (20 pts)

In this part, you'll be implementing the [Phong reflection model](https://en.wikipedia.org/wiki/Phong_reflection_model) in order to render the SDF volume you trained under different lighting conditions. In principle, the Phong model can handle multiple different light sources coming from different directions, but for our implementation we assume we're working with a single directional light source that is coming in from `light_dir` and is of unit intensity. We will feed in a dictionary of Phong parameters containing `ks, kd, ka, n`, which refer to the specular, diffuse, ambient, and shininess constants respectively. The specular, diffuse, and ambient components describe the ratio of reflection to the specular, diffuse, or ambient components of light. The shininess constant describes how smooth the surface is, with higher values making it smoother and thus shinier.

* **Surface Normal Recovery**: To relight our model, we only need to evaluate the surface normal of our volume to plug into our reflection model. This can be done by dividing the gradient by its norm and can be implemented in the `get_surface_normal` function.

* **Reflection Model**: Using the surface normals, implement the Phong reflection model in `lighting_functions.py`. To get the light direction, calculate the value in `render_images` in `main.py`.

Now, render the volume under different lighting using:

```bash
python -m a4.main --config-name=phong
```

This will save `part_4_geometry.gif` and `part_4.gif` in the `images` folder, showing your model under rotating lights.

![Bulldozer relight geometry](images/part_4_geometry.gif) ![Bulldozer relight color](images/part_4.gif)

## 5. Neural Surface Extras (CHOOSE ONE! More than one is extra credit)

### 5.1. Render a Large Scene with Sphere Tracing (10 pts)
In Q1, you rendered a (lonely) Torus, but to the power of Sphere Tracing lies in the fact that it can render complex scenes efficiently. To observe this, try defining a ‘scene’ with many (> 20) primitives (e.g. Sphere, Torus, or another SDF from [this website](https://www.iquilezles.org/www/articles/distfunctions/distfunctions.htm) at different locations). See Lecture 2 for equations of what the ‘composed’ SDF of primitives is. You can then define a new class in `implicit.py` that instantiates a complex scene with many primitives, and modify the code for Q1 to render this scene instead of a simple torus.
### 5.2 Fewer Training Views (10 pts)
In Q3, we relied on 100 training views for a single scene. A benefit of using Surface representations, however, is that the geometry is better regularized and can in principle be inferred from fewer views. Experiment with using fewer training views (say 20) -- you can do this by changing [train_idx in data laoder](https://github.com/learning3d/assignment3/blob/main/dataset.py#L123) to use a smaller random subset of indices). You should also compare the VolSDF solution to a NeRF solution learned using similar views.
### 5.3 Alternate SDF to Density Conversions (10 pts)
In Q3, we used the equations from [VolSDF Paper](https://arxiv.org/pdf/2106.12052.pdf) to convert SDF to density. You should try and compare alternate ways of doing this e.g. the ‘naive’ solution from the [NeuS paper](https://arxiv.org/pdf/2106.10689.pdf), or any other ways that you might want to propose!