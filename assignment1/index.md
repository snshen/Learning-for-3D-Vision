# 16-825 Assignment 1: Rendering Basics with PyTorch3D  
number or late days used:
<img src="./data/zero.png"  width="2%">

In this assignment, we go through the basics of rendering with PyTorch3D, explore 3D representations, and practice constructing simple geometry.

## 0. Setup
### 0.1 Rendering your first mesh
This section had no deliverables 

## 1. Practicing with Cameras 

### 1.1. 360-degree Renders (5 points)

360-degree gif video that shows many continuous views of the provided cow mesh. 

![360cow](./data/360.gif)

### 1.2 Re-creating the Dolly Zoom (10 points)

Gif with my dolly zoom effect.

![dolly](./data/dolly_zoom.gif)

## 2. Practicing with Meshes   

### 2.1 Constructing a Tetrahedron (5 points)

360-degree gif animation of my tetrahedron

![tetra](./data/360_tetra.gif)

Number of vertices: 4

Number of faces: 4

### 2.2 Constructing a Cube (5 points)

360-degree gif animation of my cube

![cube](./data/360_cube.gif)

Number of vertices: 8

Number of faces: 12

## 3. Re-texturing a mesh (10 points)

gif of the rendered mesh

![cube](./data/retexture.gif)


I used the following colors in this re-texturing exercise 


`color1`: [0, 0.5, 1] (azure)

`color2`: [1, 0.5, 0] (orange)

## 4. Camera Transformations (10 points)

The R_relative and T_relative values used and their associated outputs are listed below

![transform1](data/cow_trans1.jpg)

Rotate about z axis by pi/2, R_relative: [[cos(pi/2), -sin(pi/2), 0], [sin(pi/2), cos(pi/2), 0], [0, 0, 1]]

Use original translation, T_relative: [0, 0, 0]

![transform2](data/cow_trans2.jpg)

Use original rotation, R_relative: [[1, 0, 0], [0, 1, 0], [0, 0, 1]]

Translate along the z axis by 2, T_relative: [0, 0, 2]

![transform3](data/cow_trans3.jpg)

Use original rotation, R_relative: [[1, 0, 0], [0, 1, 0], [0, 0, 1]]

Translate along x axis by 0.5 and along y axis by -0.5, T_relative: [0.5, -0.5, 0]

![transform4](data/cow_trans4.jpg)

Rotate about y axis by -pi/2, R_relative: [[cos(-pi/2), 0, sin(-pi/2)], [0, 1, 0], [-sin(-pi/2), 0, cos(-pi/2)]]

Translate along x axis by 3 and along z axis by 3, T_relative: [3, 0, 3]


## 5. Rendering Generic 3D Representations 
### 5.1 Rendering Point Clouds from RGB-D Images (10 points)

<img src="./data/pc1.gif"  width="25%">
<img src="./data/pc2.gif"  width="25%">
<img src="./data/pc3.gif"  width="25%">

These point clouds are constructed in the following manner, from left to right:
1. The point cloud corresponding to the first image
2. The point cloud corresponding to the second image
3. The point cloud formed by the union of the first 2 point clouds

### 5.2 Parametric Functions (10 points)

360-degree gif of my torus point cloud

<img src="./data/torus.gif"  width="25%">

### 5.3 Implicit Surfaces (15 points)

360-degree gif of your torus mesh

![torus_mesh](data/torus_mesh.gif)

Tradeoffs between rendering as a mesh vs a point cloud:

The point cloud is generated by sampling a parametric function while the mesh is generated by using voxelized coordinates to query an implicit function. 

When comparing these methods, the point clouds render faster than the meshes when generating an equal number of points and voxels. This difference is due to the fact that the mesh needs to voxelize the 3D space before sampling the implicit function and apply the marching cube algorithm after sampling to extract the closest 0-level set voxel values whereas the point cloud directly uses the sampled outputs from the parametric function. 

The comparitive quality between the two representations depends on application. Individual points in a point cloud are more accurate than the mesh vertices as the voxelized output is unlikely to match the 0-level of the implicit function exactly, although this accuracy can be improved with higher resolution voxelization. However, point cloud representations do not imply surfaces and are thus difficult to texture and shade like a mesh. Additionally, meshs make assumptions about the surface of an object in between calculated vertices, which can be good for visualization and reducing noise but bad for accurately representating details on objects.

The ease of use of these representation is again dependant on application. Point clouds can be easier to compute with certain processes such as rotating or modifying the object as the computational costs for changing a set of cartesian point is less that recalculating the voxelized mesh vertices and their associated faces. However, for other processes that involve spatial awareness between points, processing unstructured point clouds can be inefficient without additionally utilizing associatied indexing or octree methods. Also, meshes provide continuous surfaces that allows for easier normal estimations compared to point clouds.

## 6. Do Something Fun (10 points)

It is time to try something fun! Here is a hoola-hooping cow for your viewing pleasures

![fun](data/fun.gif)


## (Extra Credit) 7. Sampling Points on Meshes (10 points)

I have used the cow mesh from `/data/cow.obj` folder and the joint mesh from `/data/joint_mesh.obj` folder, and randomly sample them with 100, 500, 1000, and 10000 points. Here are the following results:

1. Rendering of pointclouds and the original cow mesh side-by-side

<img src="./data/cow100.gif"  width="15%">
<img src="./data/cow500.gif"  width="15%">
<img src="./data/cow1000.gif"  width="15%">
<img src="./data/cow10000.gif"  width="15%">
<img src="./data/360.gif"  width="15%">

2. Rendering of pointclouds and the original joint_mesh side-by-side

<img src="./data/joint_mesh100.gif"  width="15%">
<img src="./data/joint_mesh500.gif"  width="15%">
<img src="./data/joint_mesh1000.gif"  width="15%">
<img src="./data/joint_mesh10000.gif"  width="15%">
<img src="./data/joint_mesh.gif"  width="15%">